{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is adapted from Sebastian Raschka's excellent blog post [Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer architecture basically eliminated the need for RNNs, mainly because of the introduction of the self-attention mechanism. What is self-attention? Self-attention is a technical approach that can help to determine not only the information of an input sequence, but also, the context of that sequence. As Raschka states: \"This is especially important for language processing tasks, where the meaning of a word can change based on its context within a sentence or document.\" "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, as linguist John Firth said in 1957: \"You shall know a word by the company it keeps.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there are many variants of self-attention, this tutorial focuses on the original scaled-dot product attention mechanim (referred to as self-attention). [Here is an overview of the scaled dot-product attention.](https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding an Input Sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Life is short, eat dessert first.'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Life': 0, 'dessert': 1, 'eat': 2, 'first.': 3, 'is': 4, 'short': 5}\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary comprehension of words and their indices in the sentence, sorted by word, and print it\n",
    "dc = {word:index for index, word in enumerate(sorted(sentence.replace(',', '').split()))} \n",
    "print(dc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the dictionary to assign an integer index to each word."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 4, 5, 2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "sentence_int =torch.tensor([dc[word] for word in sentence.replace(',', '').split()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 4, 5, 2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(sentence_int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the embedding layer using the integer-vector representation of the sentence. We use a 16-dimensional embedding, which means each input word is represented by a 16-dim vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123) # set the random seed\n",
    "embed = torch.nn.Embedding(6, 16) # 6 words in vocab, 16 dimensional embeddings\n",
    "embedded_sentence = embed(sentence_int).detach() # returns a new tensor, detached from the computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,\n",
      "          0.7671, -1.1925,  0.6984, -1.4097,  0.1794,  1.8951,  0.4954,  0.2692],\n",
      "        [ 0.5146,  0.9938, -0.2587, -1.0826, -0.0444,  1.6236, -2.3229,  1.0878,\n",
      "          0.6716,  0.6933, -0.9487, -0.0765, -0.1526,  0.1167,  0.4403, -1.4465],\n",
      "        [ 0.2553, -0.5496,  1.0042,  0.8272, -0.3948,  0.4892, -0.2168, -1.7472,\n",
      "         -1.6025, -1.0764,  0.9031, -0.7218, -0.5951, -0.7112,  0.6230, -1.3729],\n",
      "        [-1.3250,  0.1784, -2.1338,  1.0524, -0.3885, -0.9343, -0.4991, -1.0867,\n",
      "          0.8805,  1.5542,  0.6266, -0.1755,  0.0983, -0.0935,  0.2662, -0.5850],\n",
      "        [-0.0770, -1.0205, -0.1690,  0.9178,  1.5810,  1.3010,  1.2753, -0.2010,\n",
      "          0.4965, -1.5723,  0.9666, -1.1481, -1.1589,  0.3255, -0.6315, -2.8400],\n",
      "        [ 0.8768,  1.6221, -1.4779,  1.1331, -1.2203,  1.3139,  1.0533,  0.1388,\n",
      "          2.2473, -0.8036, -0.2808,  0.7697, -0.6596, -0.7979,  0.1838,  0.2293]])\n",
      "torch.Size([6, 16])\n"
     ]
    }
   ],
   "source": [
    "print(embedded_sentence)\n",
    "print(embedded_sentence.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the weight matrices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three weight matricies in self-attention (aka scaled dot product): W<sub>q</sub>, W<sub>k</sub>, and W<sub>v</sub>. As Raschka outlines, \"these matrices serve to project the inputs into query, key, and value components of the sequence, respectively.\" \n",
    "\n",
    "Q, K, and V are \"obtained via matrix multiplication between weight matrices W and the embedded inputs **x**:\n",
    "\n",
    "* Query sequence: **q**<sup>(i)</sup>=**W**<sub>q</sub>**x**<sup>(i)</sup> for i ∈[1,*T*]\n",
    "* Key sequence: **k**<sup>(i)</sup>=**W**<sub>k</sub>**x**<sup>(i)</sup> for i∈[1,*T*]\n",
    "* Value sequence: **v**<sup>(i)</sup>=**W**<sub>v</sub>**x**<sup>(i)</sup> for  i∈[1,*T*]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index *i* refers to the token index position in the input sentence, which has length *T*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the projection matrices:\n",
    "\n",
    "##### In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123) # set the random seed\n",
    "\n",
    "d = embedded_sentence.shape[1] # dimension of each word vector\n",
    "d_q, d_k, d_v = 24, 24, 28 # dimensions of query, key, and value vectors\n",
    "\n",
    "# Weights for the query, key, and value vectors. d = 16, so we have 16 weights for each of the 24, 24, and 28 vectors\n",
    "W_query = torch.rand(d_q, d) # randomly initialize query weights\n",
    "W_key = torch.rand(d_k, d) # randomly initialize key weights\n",
    "W_value = torch.rand(d_v, d) # randomly initialize value weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing the Unnormalized Attention Weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the attention-vector for the second input element.\n",
    "\n",
    "##### In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = embedded_sentence[1] # get the second word vector from the embedded sentence\n",
    "\n",
    "query_2 = W_query.matmul(x_2) # calculate the query for the second word vector. The matmul() function performs matrix multiplication.\n",
    "key_2 = W_key.matmul(x_2) # calculate the key for the second word vector\n",
    "value_2 = W_value.matmul(x_2) # calculate the value for the second word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24])\n",
      "torch.Size([24])\n",
      "torch.Size([28])\n"
     ]
    }
   ],
   "source": [
    "print(query_2.shape)\n",
    "print(key_2.shape)\n",
    "print(value_2.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, generalize this to compute the remaining key, and value elements for all inputs. We will need them in the next steps for computing the unnormalized attention weights *w:*\n",
    "\n",
    "##### In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 24])\n",
      "values.shape: torch.Size([6, 28])\n"
     ]
    }
   ],
   "source": [
    "keys = W_key.matmul(embedded_sentence.T).T # calculate the keys for all word vectors in the sentence\n",
    "values = W_value.matmul(embedded_sentence.T).T # calculate the values for all word vectors in the sentence\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the required keys and values! We can compute *w*<sub>ij</sub> as the dot product between the query and key sequences, *w*<sub>ij</sub>=**q**<sup>(i)<sup>T</sup></sup>**k**<sup>(j)</sup>.\n",
    "\n",
    "##### In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8.5808, -7.6597,  3.2558,  1.0395, 11.1466, -0.4800])\n"
     ]
    }
   ],
   "source": [
    "omega_2 = query_2.matmul(keys.T) # calculate the attention weights for the second word vector\n",
    "print(omega_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the attention scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, were are going to use the softmax function in order to obtain the normalized attention weights. We do this by applying the softmax function to the previous unnormalized attenion weights. \n",
    "\n",
    "What is the softmax function? From [deepai.org's Softmax Function article](https://deepai.org/machine-learning-glossary-and-terms/softmax-layer):\n",
    "\n",
    "*The softmax function is a function that turns a vector of K real values into a vector of K real values that sum to 1. The input values can be positive, negative, zero, or greater than one, but the softmax transforms them into values between 0 and 1, so that they can be interpreted as probabilities. If one of the inputs is small or negative, the softmax turns it into a small probability, and if an input is large, then it turns it into a large probability, but it will always remain between 0 and 1.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a scaling step for **w** before the softmax function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Raschka writes: The scaling by **d<sub>k</sub>**\n",
    " ensures that the Euclidean length of the weight vectors will be approximately in the same magnitude. This helps prevent the attention weights from becoming too small or too large, which could lead to numerical instability or affect the model’s ability to converge during training.\n",
    "\n",
    " ##### In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2912, 0.0106, 0.0982, 0.0625, 0.4917, 0.0458])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "attention_weights_2 = F.softmax(omega_2 /d_k**0.5, dim=0) # calculate the softmax of the attention weights\n",
    "print(attention_weights_2) # print the attention weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The last step is to compute the context vector **z**<sup>(2)</sup>. This is an attention-weighted verrsion of the original query input **x**<sup>(2)</sup>, including all the other input elements as its context via the attention weights:\n",
    "\n",
    "##### In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28])\n",
      "tensor([-1.5993,  0.0156,  1.2670,  0.0032, -0.6460, -1.1407, -0.4908, -1.4632,\n",
      "         0.4747,  1.1926,  0.4506, -0.7110,  0.0602,  0.7125, -0.1628, -2.0184,\n",
      "         0.3838, -2.1188, -0.8136, -1.5694,  0.7934, -0.2911, -1.3640, -0.2366,\n",
      "        -0.9564, -0.5265,  0.0624,  1.7084])\n"
     ]
    }
   ],
   "source": [
    "context_vector_2 = attention_weights_2.matmul(values) # calculate the context vector for the second word vector\n",
    "\n",
    "print(context_vector_2.shape)\n",
    "print(context_vector_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output vector has more dimensions (d<sub>v</sub> = 28) than the original input vector (d=16)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaled dot-product attention mechanism is used in the multi-head attention blocks. From the blog post:\n",
    "\n",
    "![Multi-head attention modules](images/scaled-dot-product.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*\"In the scaled dot-product attention, the input sequence was transformed using three matrices representing the query, key, and value. These three matrices can be considered as a single attention head in the context of multi-head attention.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
